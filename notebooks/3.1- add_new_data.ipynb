{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Traitement du fichier : ../data/cleaned\\2019-Dec.parquet\n",
      "- Ã‰chantillonnage de 3% des donnÃ©es dans temp_sampled\n",
      "- Attribution alÃ©atoire des user_id dans temp_sampled\n",
      "- DonnÃ©es insÃ©rÃ©es dans final_sampled\n",
      "\n",
      "Traitement du fichier : ../data/cleaned\\2019-Nov.parquet\n",
      "- Ã‰chantillonnage de 3% des donnÃ©es dans temp_sampled\n",
      "- Attribution alÃ©atoire des user_id dans temp_sampled\n",
      "- DonnÃ©es insÃ©rÃ©es dans final_sampled\n",
      "\n",
      "Traitement du fichier : ../data/cleaned\\2019-Oct.parquet\n",
      "- Ã‰chantillonnage de 3% des donnÃ©es dans temp_sampled\n",
      "- Attribution alÃ©atoire des user_id dans temp_sampled\n",
      "- DonnÃ©es insÃ©rÃ©es dans final_sampled\n",
      "\n",
      "Traitement du fichier : ../data/cleaned\\2020-Apr.parquet\n",
      "- Ã‰chantillonnage de 3% des donnÃ©es dans temp_sampled\n",
      "- Attribution alÃ©atoire des user_id dans temp_sampled\n",
      "- DonnÃ©es insÃ©rÃ©es dans final_sampled\n",
      "\n",
      "Traitement du fichier : ../data/cleaned\\2020-Feb.parquet\n",
      "- Ã‰chantillonnage de 3% des donnÃ©es dans temp_sampled\n",
      "- Attribution alÃ©atoire des user_id dans temp_sampled\n",
      "- DonnÃ©es insÃ©rÃ©es dans final_sampled\n",
      "\n",
      "Traitement du fichier : ../data/cleaned\\2020-Jan.parquet\n",
      "- Ã‰chantillonnage de 3% des donnÃ©es dans temp_sampled\n",
      "- Attribution alÃ©atoire des user_id dans temp_sampled\n",
      "- DonnÃ©es insÃ©rÃ©es dans final_sampled\n",
      "\n",
      "Traitement du fichier : ../data/cleaned\\2020-Mar.parquet\n",
      "- Ã‰chantillonnage de 3% des donnÃ©es dans temp_sampled\n",
      "- Attribution alÃ©atoire des user_id dans temp_sampled\n",
      "- DonnÃ©es insÃ©rÃ©es dans final_sampled\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 0\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 100000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 200000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 300000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 400000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 500000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 600000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 700000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 800000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 900000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 1000000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 1100000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 1200000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 1300000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 1400000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 1500000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 1600000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 1700000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 1800000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 1900000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 2000000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 2100000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 2200000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 2300000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 2400000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 2500000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 2600000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 2700000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 2800000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 2900000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 3000000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 3100000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 3200000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 3300000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 3400000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 3500000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 3600000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 3700000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 3800000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 3900000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 4000000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 4100000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 4200000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 4300000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 4400000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 4500000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 4600000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 4700000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 4800000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 4900000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 5000000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 5100000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 5200000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 5300000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 5400000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 5500000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 5600000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 5700000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 5800000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 5900000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 6000000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 6100000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 6200000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 6300000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 6400000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 6500000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 6600000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 6700000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 6800000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 6900000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 7000000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 7100000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 7200000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 7300000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 7400000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 7500000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 7600000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 7700000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 7800000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 7900000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 8000000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 8100000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 8200000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 8300000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 8400000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 8500000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 8600000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 8700000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 8800000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 8900000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 9000000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 9100000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 9200000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 9300000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 9400000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 9500000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 9600000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 9700000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 9800000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 9900000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 10000000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 10100000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 10200000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 10300000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 10400000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 10500000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 10600000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 10700000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 10800000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 10900000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 11000000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 11100000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 11200000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 11300000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 11400000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 11500000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 11600000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 11700000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 11800000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 11900000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 12000000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 12100000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 12200000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 12300000\n",
      "- ExportÃ© 100000 lignes Ã  partir de l'offset 12400000\n",
      "- ExportÃ© 7136 lignes Ã  partir de l'offset 12500000\n",
      "\n",
      "Fichier sauvegardÃ© : ../data/output/data-sampled.parquet\n"
     ]
    }
   ],
   "source": [
    "# Chemin vers les fichiers\n",
    "input_folder = \"../data/cleaned/\"\n",
    "output_path = \"../data/output/data-sampled.parquet\"\n",
    "\n",
    "# Lire tous les fichiers .parquet dans le dossier\n",
    "file_paths = glob.glob(f\"{input_folder}/*.parquet\")\n",
    "\n",
    "# Initialiser une base de donnÃ©es en mÃ©moire\n",
    "db = duckdb.connect(database=':memory:')\n",
    "\n",
    "# Initialisation de la table finale dynamique (en fonction des colonnes du premier fichier)\n",
    "first_file = file_paths[0]\n",
    "db.execute(f\"CREATE TABLE final_sampled AS SELECT * FROM parquet_scan('{first_file}') LIMIT 0\")\n",
    "\n",
    "# Liste pour accumuler les donnÃ©es par morceaux\n",
    "batch_writer = None\n",
    "\n",
    "for file_path in file_paths:\n",
    "    print(f\"\\nTraitement du fichier : {file_path}\")\n",
    "\n",
    "    # Supprimer la table temporaire si elle existe dÃ©jÃ \n",
    "    db.execute(\"DROP TABLE IF EXISTS temp_sampled\")\n",
    "\n",
    "    # Ã‰chantillonnage direct sans charger tout le fichier en mÃ©moire\n",
    "    db.execute(f\"\"\"\n",
    "        CREATE TEMP TABLE temp_sampled AS\n",
    "        SELECT * FROM parquet_scan('{file_path}') USING SAMPLE 3%\n",
    "    \"\"\")\n",
    "    print(\"- Ã‰chantillonnage de 3% des donnÃ©es dans temp_sampled\")\n",
    "\n",
    "    # Extraire et stocker user_id dans un DataFrame\n",
    "    user_ids = db.execute(\"SELECT DISTINCT user_id FROM temp_sampled WHERE user_id IS NOT NULL\").fetch_df()['user_id'].values\n",
    "\n",
    "    if len(user_ids) > 0:\n",
    "        np.random.shuffle(user_ids)\n",
    "\n",
    "        # Extraire les lignes oÃ¹ user_id est NULL\n",
    "        null_user_ids = db.execute(\"SELECT rowid FROM temp_sampled WHERE user_id IS NULL\").fetch_df()['rowid'].values\n",
    "\n",
    "        # Assigner un user_id alÃ©atoire unique Ã  chaque ligne oÃ¹ user_id est NULL\n",
    "        for rowid in null_user_ids:\n",
    "            random_user_id = np.random.choice(user_ids)\n",
    "            db.execute(f\"UPDATE temp_sampled SET user_id = '{random_user_id}' WHERE rowid = {rowid}\")\n",
    "\n",
    "        print(\"- Attribution alÃ©atoire des user_id dans temp_sampled\")\n",
    "    else:\n",
    "        print(\"- Avertissement : Aucun user_id valide trouvÃ© dans temp_sampled\")\n",
    "\n",
    "    # InsÃ©rer les donnÃ©es Ã©chantillonnÃ©es dans la table finale par lots\n",
    "    db.execute(\"INSERT INTO final_sampled SELECT * FROM temp_sampled\")\n",
    "    print(\"- DonnÃ©es insÃ©rÃ©es dans final_sampled\")\n",
    "\n",
    "# Exporter le rÃ©sultat final sans tout charger en mÃ©moire\n",
    "# Ã‰crire les rÃ©sultats par petits morceaux\n",
    "chunk_size = 100000  # Taille des morceaux Ã  exporter\n",
    "offset = 0\n",
    "\n",
    "while True:\n",
    "    result = db.execute(f\"SELECT * FROM final_sampled WHERE user_id IS NOT NULL LIMIT {chunk_size} OFFSET {offset}\").fetch_df()\n",
    "\n",
    "    if result.empty:\n",
    "        break\n",
    "\n",
    "    # Convertir le DataFrame Pandas en table Arrow pour l'Ã©criture Parquet\n",
    "    table = pa.Table.from_pandas(result)\n",
    "\n",
    "    # Si le fichier n'existe pas, crÃ©er un nouveau fichier Parquet\n",
    "    if batch_writer is None:\n",
    "        batch_writer = pq.ParquetWriter(output_path, table.schema)\n",
    "\n",
    "    # Ajouter le lot au fichier Parquet\n",
    "    batch_writer.write_table(table)\n",
    "    print(f\"- ExportÃ© {len(result)} lignes Ã  partir de l'offset {offset}\")\n",
    "\n",
    "    offset += chunk_size\n",
    "\n",
    "# Fermer l'Ã©crivain Parquet pour finaliser l'Ã©criture\n",
    "if batch_writer:\n",
    "    batch_writer.close()\n",
    "\n",
    "print(f\"\\nFichier sauvegardÃ© : {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“‚ DÃ©finition des chemins\n",
    "DATA_PATH = \"../data/output/data-sampled\"\n",
    "OUTPUT_PATH = \"../data/output/user_features_data-sampled.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Chargement des donnÃ©es en streaming...\n",
      "ğŸ“‚ Chargement des fichiers Parquet en streaming avec DuckDB...\n",
      "ğŸ“Š Calcul des mÃ©triques par utilisateur...\n",
      "ğŸ“… Conversion des dates...\n",
      "ğŸ”¢ Calcul du taux de conversion...\n",
      "ğŸ”¢ Calcul du taux de panier transformÃ©...\n",
      "â³ Calcul du temps depuis le dernier Ã©vÃ©nement...\n",
      "ğŸ’¾ Sauvegarde optimisÃ©e du fichier ../data/output/user_features_data-sampled.parquet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pierre\\AppData\\Local\\Temp\\ipykernel_2872\\2321329801.py:53: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  user_features[\"conversion_rate_view\"].fillna(0, inplace=True)\n",
      "C:\\Users\\Pierre\\AppData\\Local\\Temp\\ipykernel_2872\\2321329801.py:58: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  user_features[\"conversion_rate_cart\"].fillna(0, inplace=True)\n",
      "C:\\Users\\Pierre\\AppData\\Local\\Temp\\ipykernel_2872\\2321329801.py:62: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  now_utc = datetime.utcnow()\n",
      "C:\\Users\\Pierre\\AppData\\Local\\Temp\\ipykernel_2872\\2321329801.py:69: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  user_features.fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Transformation terminÃ©e avec succÃ¨s !\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“Œ CatÃ©gories sÃ©lectionnÃ©es\n",
    "SELECTED_CATEGORIES = {\"electronics\", \"computers\", \"sport\", \"kids\"}\n",
    "\n",
    "def load_data_with_duckdb():\n",
    "    \"\"\"Charge et filtre les donnÃ©es en streaming avec DuckDB pour Ã©conomiser la mÃ©moire.\"\"\"\n",
    "    print(\"ğŸ“‚ Chargement des fichiers Parquet en streaming avec DuckDB...\")\n",
    "\n",
    "    query = f\"\"\"\n",
    "        SELECT \n",
    "            user_id, \n",
    "            event_type, \n",
    "            event_time,\n",
    "            price, \n",
    "            split_part(category_code, '.', 1) AS category_main\n",
    "        FROM read_parquet('{DATA_PATH}*.parquet', hive_partitioning=0)\n",
    "        WHERE event_type IN ('view', 'purchase','cart')\n",
    "        AND split_part(category_code, '.', 1) IN ({', '.join(f\"'{cat}'\" for cat in SELECTED_CATEGORIES)})\n",
    "    \"\"\"\n",
    "\n",
    "    return duckdb.query(query)  # Retourne un objet DuckDB en streaming\n",
    "\n",
    "def generate_user_features():\n",
    "    \"\"\"Transforme les donnÃ©es en jeu de caractÃ©ristiques par utilisateur avec DuckDB en optimisant la mÃ©moire.\"\"\"\n",
    "    print(\"ğŸ”„ Chargement des donnÃ©es en streaming...\")\n",
    "    df = load_data_with_duckdb()  # Chargement optimisÃ©\n",
    "\n",
    "    print(\"ğŸ“Š Calcul des mÃ©triques par utilisateur...\")\n",
    "\n",
    "    # ğŸ“Œ AgrÃ©gation en plusieurs Ã©tapes pour rÃ©duire la mÃ©moire utilisÃ©e\n",
    "    user_features_query = \"\"\"\n",
    "        SELECT \n",
    "            user_id,\n",
    "            COUNT(*) AS total_events,\n",
    "            COUNT(CASE WHEN event_type = 'view' THEN 1 END) AS total_views,\n",
    "            COUNT(CASE WHEN event_type = 'cart' THEN 1 END) AS total_carts,\n",
    "            COUNT(CASE WHEN event_type = 'purchase' THEN 1 END) AS total_purchases,\n",
    "            SUM(CASE WHEN event_type = 'purchase' THEN price ELSE 0 END) AS total_spent,\n",
    "            COUNT(DISTINCT category_main) AS unique_categories,\n",
    "            MAX(event_time) AS last_event_time\n",
    "        FROM df\n",
    "        GROUP BY user_id\n",
    "    \"\"\"\n",
    "\n",
    "    user_features = duckdb.query(user_features_query).to_df()\n",
    "\n",
    "    print(\"ğŸ“… Conversion des dates...\")\n",
    "    # ğŸ“Œ Conversion des dates (en UTC et naive)\n",
    "    user_features[\"last_event_time\"] = pd.to_datetime(user_features[\"last_event_time\"]).dt.tz_localize(None)\n",
    "\n",
    "    print(\"ğŸ”¢ Calcul du taux de conversion...\")\n",
    "    # ğŸ“Œ Calcul du taux de conversion\n",
    "    user_features[\"conversion_rate_view\"] = user_features[\"total_purchases\"] / user_features[\"total_views\"]\n",
    "    user_features[\"conversion_rate_view\"].fillna(0, inplace=True)\n",
    "\n",
    "    print(\"ğŸ”¢ Calcul du taux de panier transformÃ©...\")\n",
    "    # ğŸ“Œ Calcul du taux de conversion\n",
    "    user_features[\"conversion_rate_cart\"] = user_features[\"total_purchases\"] / user_features[\"total_carts\"]\n",
    "    user_features[\"conversion_rate_cart\"].fillna(0, inplace=True)\n",
    "\n",
    "    print(\"â³ Calcul du temps depuis le dernier Ã©vÃ©nement...\")\n",
    "    # ğŸ“Œ Calcul du temps depuis le dernier Ã©vÃ©nement (en jours)\n",
    "    now_utc = datetime.utcnow()\n",
    "    user_features[\"days_since_last_event\"] = (\n",
    "        now_utc - user_features[\"last_event_time\"]\n",
    "    ).dt.total_seconds() / (3600 * 24)\n",
    "\n",
    "    # ğŸ“Œ Nettoyage des valeurs infinies et NaN\n",
    "    user_features.replace([float(\"inf\"), float(\"-inf\")], None, inplace=True)\n",
    "    user_features.fillna(0, inplace=True)\n",
    "\n",
    "    # ğŸ“Œ Sauvegarde avec compression (pour Ã©conomiser la RAM)\n",
    "    print(f\"ğŸ’¾ Sauvegarde optimisÃ©e du fichier {OUTPUT_PATH}...\")\n",
    "    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
    "    user_features.to_parquet(OUTPUT_PATH, engine=\"pyarrow\", compression=\"snappy\")\n",
    "\n",
    "    print(\"âœ… Transformation terminÃ©e avec succÃ¨s !\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_user_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ Chargement des modÃ¨les sauvegardÃ©s...\n",
      "âœ… ModÃ¨les chargÃ©s : scaler, PCA, et K-Means.\n",
      "ğŸ“¥ Chargement des nouvelles donnÃ©es...\n",
      "âœ… Nouvelles donnÃ©es chargÃ©es : 814509 lignes, 11 colonnes.\n",
      "ğŸ”„ Normalisation des nouvelles donnÃ©es...\n",
      "âœ… Normalisation terminÃ©e.\n",
      "ğŸ”„ RÃ©duction de dimension avec l'ACP...\n",
      "âœ… RÃ©duction de dimension terminÃ©e.\n",
      "ğŸ¯ PrÃ©diction des clusters...\n",
      "âœ… PrÃ©diction des clusters terminÃ©e.\n",
      "ğŸ’¾ Sauvegarde des rÃ©sultats...\n",
      "âœ… RÃ©sultats sauvegardÃ©s dans '../data/output/new_user_clusters-test.parquet'.\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¹ Charger les modÃ¨les sauvegardÃ©s\n",
    "print(\"ğŸ“¥ Chargement des modÃ¨les sauvegardÃ©s...\")\n",
    "scaler = joblib.load(\"../data/output/scaler-test.pkl\")\n",
    "pca = joblib.load(\"../data/output/pca_model-test.pkl\")\n",
    "kmeans = joblib.load(\"../data/output/kmeans_model-test.pkl\")\n",
    "print(\"âœ… ModÃ¨les chargÃ©s : scaler, PCA, et K-Means.\")\n",
    "\n",
    "# ğŸ”¹ Charger les nouvelles donnÃ©es\n",
    "print(\"ğŸ“¥ Chargement des nouvelles donnÃ©es...\")\n",
    "new_data = pd.read_parquet(\"../data/output/user_features_data-sampled.parquet\")\n",
    "print(f\"âœ… Nouvelles donnÃ©es chargÃ©es : {new_data.shape[0]} lignes, {new_data.shape[1]} colonnes.\")\n",
    "\n",
    "# ğŸ”¹ Normaliser les nouvelles donnÃ©es\n",
    "print(\"ğŸ”„ Normalisation des nouvelles donnÃ©es...\")\n",
    "features = [\"total_events\", \"total_views\", \"total_carts\", \"total_purchases\", \n",
    "            \"total_spent\", \"unique_categories\", \"days_since_last_event\"]\n",
    "new_data_scaled = scaler.transform(new_data[features])  # Appliquer le scaler sauvegardÃ©\n",
    "print(\"âœ… Normalisation terminÃ©e.\")\n",
    "\n",
    "# ğŸ”¹ RÃ©duction de dimension avec l'ACP\n",
    "print(\"ğŸ”„ RÃ©duction de dimension avec l'ACP...\")\n",
    "new_data_pca = pca.transform(new_data_scaled)  # Appliquer le modÃ¨le PCA sauvegardÃ©\n",
    "new_data_pca_df = pd.DataFrame(data=new_data_pca, columns=[\"PC1\", \"PC2\"])\n",
    "new_data_pca_df[\"user_id\"] = new_data[\"user_id\"]\n",
    "print(\"âœ… RÃ©duction de dimension terminÃ©e.\")\n",
    "\n",
    "# ğŸ”¹ PrÃ©dire les clusters\n",
    "print(\"ğŸ¯ PrÃ©diction des clusters...\")\n",
    "new_data_pca_df[\"Cluster\"] = kmeans.predict(new_data_pca_df[[\"PC1\", \"PC2\"]])  # Appliquer le modÃ¨le K-Means\n",
    "print(\"âœ… PrÃ©diction des clusters terminÃ©e.\")\n",
    "\n",
    "# ğŸ”¹ Sauvegarder les rÃ©sultats\n",
    "print(\"ğŸ’¾ Sauvegarde des rÃ©sultats...\")\n",
    "new_data_pca_df.to_parquet(\"../data/output/new_user_clusters-test.parquet\", index=False)\n",
    "print(\"âœ… RÃ©sultats sauvegardÃ©s dans '../data/output/new_user_clusters-test.parquet'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Cluster   count\n",
      "0        0   49107\n",
      "1        1     707\n",
      "2        2  764218\n",
      "3        3     477\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "# Chemin vers votre fichier Parquet\n",
    "file_path = \"../data/output/new_user_clusters-test.parquet\"\n",
    "\n",
    "try:\n",
    "    # Ã‰tablir une connexion Ã  DuckDB\n",
    "    con = duckdb.connect(database=':memory:', read_only=False)\n",
    "\n",
    "    # ExÃ©cuter la requÃªte SQL pour faire le group by et compter les lignes\n",
    "    result = con.execute(f\"SELECT Cluster, COUNT(*) AS count FROM '{file_path}' GROUP BY Cluster\").fetchdf()\n",
    "\n",
    "    # Afficher le rÃ©sultat\n",
    "    print(result)\n",
    "\n",
    "    # Fermer la connexion DuckDB\n",
    "    con.close()\n",
    "\n",
    "except duckdb.Error as e:\n",
    "    print(f\"Une erreur DuckDB s'est produite : {e}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Le fichier '{file_path}' n'a pas Ã©tÃ© trouvÃ©.\")\n",
    "except Exception as e:\n",
    "    print(f\"Une erreur inattendue s'est produite : {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
