{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Traitement du fichier : ../data/cleaned\\2019-Dec.parquet\n",
      "- Échantillonnage de 3% des données dans temp_sampled\n",
      "- Attribution aléatoire des user_id dans temp_sampled\n",
      "- Données insérées dans final_sampled\n",
      "\n",
      "Traitement du fichier : ../data/cleaned\\2019-Nov.parquet\n",
      "- Échantillonnage de 3% des données dans temp_sampled\n",
      "- Attribution aléatoire des user_id dans temp_sampled\n",
      "- Données insérées dans final_sampled\n",
      "\n",
      "Traitement du fichier : ../data/cleaned\\2019-Oct.parquet\n",
      "- Échantillonnage de 3% des données dans temp_sampled\n",
      "- Attribution aléatoire des user_id dans temp_sampled\n",
      "- Données insérées dans final_sampled\n",
      "\n",
      "Traitement du fichier : ../data/cleaned\\2020-Apr.parquet\n",
      "- Échantillonnage de 3% des données dans temp_sampled\n",
      "- Attribution aléatoire des user_id dans temp_sampled\n",
      "- Données insérées dans final_sampled\n",
      "\n",
      "Traitement du fichier : ../data/cleaned\\2020-Feb.parquet\n",
      "- Échantillonnage de 3% des données dans temp_sampled\n",
      "- Attribution aléatoire des user_id dans temp_sampled\n",
      "- Données insérées dans final_sampled\n",
      "\n",
      "Traitement du fichier : ../data/cleaned\\2020-Jan.parquet\n",
      "- Échantillonnage de 3% des données dans temp_sampled\n",
      "- Attribution aléatoire des user_id dans temp_sampled\n",
      "- Données insérées dans final_sampled\n",
      "\n",
      "Traitement du fichier : ../data/cleaned\\2020-Mar.parquet\n",
      "- Échantillonnage de 3% des données dans temp_sampled\n",
      "- Attribution aléatoire des user_id dans temp_sampled\n",
      "- Données insérées dans final_sampled\n",
      "- Exporté 100000 lignes à partir de l'offset 0\n",
      "- Exporté 100000 lignes à partir de l'offset 100000\n",
      "- Exporté 100000 lignes à partir de l'offset 200000\n",
      "- Exporté 100000 lignes à partir de l'offset 300000\n",
      "- Exporté 100000 lignes à partir de l'offset 400000\n",
      "- Exporté 100000 lignes à partir de l'offset 500000\n",
      "- Exporté 100000 lignes à partir de l'offset 600000\n",
      "- Exporté 100000 lignes à partir de l'offset 700000\n",
      "- Exporté 100000 lignes à partir de l'offset 800000\n",
      "- Exporté 100000 lignes à partir de l'offset 900000\n",
      "- Exporté 100000 lignes à partir de l'offset 1000000\n",
      "- Exporté 100000 lignes à partir de l'offset 1100000\n",
      "- Exporté 100000 lignes à partir de l'offset 1200000\n",
      "- Exporté 100000 lignes à partir de l'offset 1300000\n",
      "- Exporté 100000 lignes à partir de l'offset 1400000\n",
      "- Exporté 100000 lignes à partir de l'offset 1500000\n",
      "- Exporté 100000 lignes à partir de l'offset 1600000\n",
      "- Exporté 100000 lignes à partir de l'offset 1700000\n",
      "- Exporté 100000 lignes à partir de l'offset 1800000\n",
      "- Exporté 100000 lignes à partir de l'offset 1900000\n",
      "- Exporté 100000 lignes à partir de l'offset 2000000\n",
      "- Exporté 100000 lignes à partir de l'offset 2100000\n",
      "- Exporté 100000 lignes à partir de l'offset 2200000\n",
      "- Exporté 100000 lignes à partir de l'offset 2300000\n",
      "- Exporté 100000 lignes à partir de l'offset 2400000\n",
      "- Exporté 100000 lignes à partir de l'offset 2500000\n",
      "- Exporté 100000 lignes à partir de l'offset 2600000\n",
      "- Exporté 100000 lignes à partir de l'offset 2700000\n",
      "- Exporté 100000 lignes à partir de l'offset 2800000\n",
      "- Exporté 100000 lignes à partir de l'offset 2900000\n",
      "- Exporté 100000 lignes à partir de l'offset 3000000\n",
      "- Exporté 100000 lignes à partir de l'offset 3100000\n",
      "- Exporté 100000 lignes à partir de l'offset 3200000\n",
      "- Exporté 100000 lignes à partir de l'offset 3300000\n",
      "- Exporté 100000 lignes à partir de l'offset 3400000\n",
      "- Exporté 100000 lignes à partir de l'offset 3500000\n",
      "- Exporté 100000 lignes à partir de l'offset 3600000\n",
      "- Exporté 100000 lignes à partir de l'offset 3700000\n",
      "- Exporté 100000 lignes à partir de l'offset 3800000\n",
      "- Exporté 100000 lignes à partir de l'offset 3900000\n",
      "- Exporté 100000 lignes à partir de l'offset 4000000\n",
      "- Exporté 100000 lignes à partir de l'offset 4100000\n",
      "- Exporté 100000 lignes à partir de l'offset 4200000\n",
      "- Exporté 100000 lignes à partir de l'offset 4300000\n",
      "- Exporté 100000 lignes à partir de l'offset 4400000\n",
      "- Exporté 100000 lignes à partir de l'offset 4500000\n",
      "- Exporté 100000 lignes à partir de l'offset 4600000\n",
      "- Exporté 100000 lignes à partir de l'offset 4700000\n",
      "- Exporté 100000 lignes à partir de l'offset 4800000\n",
      "- Exporté 100000 lignes à partir de l'offset 4900000\n",
      "- Exporté 100000 lignes à partir de l'offset 5000000\n",
      "- Exporté 100000 lignes à partir de l'offset 5100000\n",
      "- Exporté 100000 lignes à partir de l'offset 5200000\n",
      "- Exporté 100000 lignes à partir de l'offset 5300000\n",
      "- Exporté 100000 lignes à partir de l'offset 5400000\n",
      "- Exporté 100000 lignes à partir de l'offset 5500000\n",
      "- Exporté 100000 lignes à partir de l'offset 5600000\n",
      "- Exporté 100000 lignes à partir de l'offset 5700000\n",
      "- Exporté 100000 lignes à partir de l'offset 5800000\n",
      "- Exporté 100000 lignes à partir de l'offset 5900000\n",
      "- Exporté 100000 lignes à partir de l'offset 6000000\n",
      "- Exporté 100000 lignes à partir de l'offset 6100000\n",
      "- Exporté 100000 lignes à partir de l'offset 6200000\n",
      "- Exporté 100000 lignes à partir de l'offset 6300000\n",
      "- Exporté 100000 lignes à partir de l'offset 6400000\n",
      "- Exporté 100000 lignes à partir de l'offset 6500000\n",
      "- Exporté 100000 lignes à partir de l'offset 6600000\n",
      "- Exporté 100000 lignes à partir de l'offset 6700000\n",
      "- Exporté 100000 lignes à partir de l'offset 6800000\n",
      "- Exporté 100000 lignes à partir de l'offset 6900000\n",
      "- Exporté 100000 lignes à partir de l'offset 7000000\n",
      "- Exporté 100000 lignes à partir de l'offset 7100000\n",
      "- Exporté 100000 lignes à partir de l'offset 7200000\n",
      "- Exporté 100000 lignes à partir de l'offset 7300000\n",
      "- Exporté 100000 lignes à partir de l'offset 7400000\n",
      "- Exporté 100000 lignes à partir de l'offset 7500000\n",
      "- Exporté 100000 lignes à partir de l'offset 7600000\n",
      "- Exporté 100000 lignes à partir de l'offset 7700000\n",
      "- Exporté 100000 lignes à partir de l'offset 7800000\n",
      "- Exporté 100000 lignes à partir de l'offset 7900000\n",
      "- Exporté 100000 lignes à partir de l'offset 8000000\n",
      "- Exporté 100000 lignes à partir de l'offset 8100000\n",
      "- Exporté 100000 lignes à partir de l'offset 8200000\n",
      "- Exporté 100000 lignes à partir de l'offset 8300000\n",
      "- Exporté 100000 lignes à partir de l'offset 8400000\n",
      "- Exporté 100000 lignes à partir de l'offset 8500000\n",
      "- Exporté 100000 lignes à partir de l'offset 8600000\n",
      "- Exporté 100000 lignes à partir de l'offset 8700000\n",
      "- Exporté 100000 lignes à partir de l'offset 8800000\n",
      "- Exporté 100000 lignes à partir de l'offset 8900000\n",
      "- Exporté 100000 lignes à partir de l'offset 9000000\n",
      "- Exporté 100000 lignes à partir de l'offset 9100000\n",
      "- Exporté 100000 lignes à partir de l'offset 9200000\n",
      "- Exporté 100000 lignes à partir de l'offset 9300000\n",
      "- Exporté 100000 lignes à partir de l'offset 9400000\n",
      "- Exporté 100000 lignes à partir de l'offset 9500000\n",
      "- Exporté 100000 lignes à partir de l'offset 9600000\n",
      "- Exporté 100000 lignes à partir de l'offset 9700000\n",
      "- Exporté 100000 lignes à partir de l'offset 9800000\n",
      "- Exporté 100000 lignes à partir de l'offset 9900000\n",
      "- Exporté 100000 lignes à partir de l'offset 10000000\n",
      "- Exporté 100000 lignes à partir de l'offset 10100000\n",
      "- Exporté 100000 lignes à partir de l'offset 10200000\n",
      "- Exporté 100000 lignes à partir de l'offset 10300000\n",
      "- Exporté 100000 lignes à partir de l'offset 10400000\n",
      "- Exporté 100000 lignes à partir de l'offset 10500000\n",
      "- Exporté 100000 lignes à partir de l'offset 10600000\n",
      "- Exporté 100000 lignes à partir de l'offset 10700000\n",
      "- Exporté 100000 lignes à partir de l'offset 10800000\n",
      "- Exporté 100000 lignes à partir de l'offset 10900000\n",
      "- Exporté 100000 lignes à partir de l'offset 11000000\n",
      "- Exporté 100000 lignes à partir de l'offset 11100000\n",
      "- Exporté 100000 lignes à partir de l'offset 11200000\n",
      "- Exporté 100000 lignes à partir de l'offset 11300000\n",
      "- Exporté 100000 lignes à partir de l'offset 11400000\n",
      "- Exporté 100000 lignes à partir de l'offset 11500000\n",
      "- Exporté 100000 lignes à partir de l'offset 11600000\n",
      "- Exporté 100000 lignes à partir de l'offset 11700000\n",
      "- Exporté 100000 lignes à partir de l'offset 11800000\n",
      "- Exporté 100000 lignes à partir de l'offset 11900000\n",
      "- Exporté 100000 lignes à partir de l'offset 12000000\n",
      "- Exporté 100000 lignes à partir de l'offset 12100000\n",
      "- Exporté 100000 lignes à partir de l'offset 12200000\n",
      "- Exporté 100000 lignes à partir de l'offset 12300000\n",
      "- Exporté 100000 lignes à partir de l'offset 12400000\n",
      "- Exporté 7136 lignes à partir de l'offset 12500000\n",
      "\n",
      "Fichier sauvegardé : ../data/output/data-sampled.parquet\n"
     ]
    }
   ],
   "source": [
    "# Chemin vers les fichiers\n",
    "input_folder = \"../data/cleaned/\"\n",
    "output_path = \"../data/output/data-sampled.parquet\"\n",
    "\n",
    "# Lire tous les fichiers .parquet dans le dossier\n",
    "file_paths = glob.glob(f\"{input_folder}/*.parquet\")\n",
    "\n",
    "# Initialiser une base de données en mémoire\n",
    "db = duckdb.connect(database=':memory:')\n",
    "\n",
    "# Initialisation de la table finale dynamique (en fonction des colonnes du premier fichier)\n",
    "first_file = file_paths[0]\n",
    "db.execute(f\"CREATE TABLE final_sampled AS SELECT * FROM parquet_scan('{first_file}') LIMIT 0\")\n",
    "\n",
    "# Liste pour accumuler les données par morceaux\n",
    "batch_writer = None\n",
    "\n",
    "for file_path in file_paths:\n",
    "    print(f\"\\nTraitement du fichier : {file_path}\")\n",
    "\n",
    "    # Supprimer la table temporaire si elle existe déjà\n",
    "    db.execute(\"DROP TABLE IF EXISTS temp_sampled\")\n",
    "\n",
    "    # Échantillonnage direct sans charger tout le fichier en mémoire\n",
    "    db.execute(f\"\"\"\n",
    "        CREATE TEMP TABLE temp_sampled AS\n",
    "        SELECT * FROM parquet_scan('{file_path}') USING SAMPLE 3%\n",
    "    \"\"\")\n",
    "    print(\"- Échantillonnage de 3% des données dans temp_sampled\")\n",
    "\n",
    "    # Extraire et stocker user_id dans un DataFrame\n",
    "    user_ids = db.execute(\"SELECT DISTINCT user_id FROM temp_sampled WHERE user_id IS NOT NULL\").fetch_df()['user_id'].values\n",
    "\n",
    "    if len(user_ids) > 0:\n",
    "        np.random.shuffle(user_ids)\n",
    "\n",
    "        # Extraire les lignes où user_id est NULL\n",
    "        null_user_ids = db.execute(\"SELECT rowid FROM temp_sampled WHERE user_id IS NULL\").fetch_df()['rowid'].values\n",
    "\n",
    "        # Assigner un user_id aléatoire unique à chaque ligne où user_id est NULL\n",
    "        for rowid in null_user_ids:\n",
    "            random_user_id = np.random.choice(user_ids)\n",
    "            db.execute(f\"UPDATE temp_sampled SET user_id = '{random_user_id}' WHERE rowid = {rowid}\")\n",
    "\n",
    "        print(\"- Attribution aléatoire des user_id dans temp_sampled\")\n",
    "    else:\n",
    "        print(\"- Avertissement : Aucun user_id valide trouvé dans temp_sampled\")\n",
    "\n",
    "    # Insérer les données échantillonnées dans la table finale par lots\n",
    "    db.execute(\"INSERT INTO final_sampled SELECT * FROM temp_sampled\")\n",
    "    print(\"- Données insérées dans final_sampled\")\n",
    "\n",
    "# Exporter le résultat final sans tout charger en mémoire\n",
    "# Écrire les résultats par petits morceaux\n",
    "chunk_size = 100000  # Taille des morceaux à exporter\n",
    "offset = 0\n",
    "\n",
    "while True:\n",
    "    result = db.execute(f\"SELECT * FROM final_sampled WHERE user_id IS NOT NULL LIMIT {chunk_size} OFFSET {offset}\").fetch_df()\n",
    "\n",
    "    if result.empty:\n",
    "        break\n",
    "\n",
    "    # Convertir le DataFrame Pandas en table Arrow pour l'écriture Parquet\n",
    "    table = pa.Table.from_pandas(result)\n",
    "\n",
    "    # Si le fichier n'existe pas, créer un nouveau fichier Parquet\n",
    "    if batch_writer is None:\n",
    "        batch_writer = pq.ParquetWriter(output_path, table.schema)\n",
    "\n",
    "    # Ajouter le lot au fichier Parquet\n",
    "    batch_writer.write_table(table)\n",
    "    print(f\"- Exporté {len(result)} lignes à partir de l'offset {offset}\")\n",
    "\n",
    "    offset += chunk_size\n",
    "\n",
    "# Fermer l'écrivain Parquet pour finaliser l'écriture\n",
    "if batch_writer:\n",
    "    batch_writer.close()\n",
    "\n",
    "print(f\"\\nFichier sauvegardé : {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📂 Définition des chemins\n",
    "DATA_PATH = \"../data/output/data-sampled\"\n",
    "OUTPUT_PATH = \"../data/output/user_features_data-sampled.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Chargement des données en streaming...\n",
      "📂 Chargement des fichiers Parquet en streaming avec DuckDB...\n",
      "📊 Calcul des métriques par utilisateur...\n",
      "📅 Conversion des dates...\n",
      "🔢 Calcul du taux de conversion...\n",
      "🔢 Calcul du taux de panier transformé...\n",
      "⏳ Calcul du temps depuis le dernier événement...\n",
      "💾 Sauvegarde optimisée du fichier ../data/output/user_features_data-sampled.parquet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pierre\\AppData\\Local\\Temp\\ipykernel_2872\\2321329801.py:53: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  user_features[\"conversion_rate_view\"].fillna(0, inplace=True)\n",
      "C:\\Users\\Pierre\\AppData\\Local\\Temp\\ipykernel_2872\\2321329801.py:58: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  user_features[\"conversion_rate_cart\"].fillna(0, inplace=True)\n",
      "C:\\Users\\Pierre\\AppData\\Local\\Temp\\ipykernel_2872\\2321329801.py:62: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  now_utc = datetime.utcnow()\n",
      "C:\\Users\\Pierre\\AppData\\Local\\Temp\\ipykernel_2872\\2321329801.py:69: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  user_features.fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Transformation terminée avec succès !\n"
     ]
    }
   ],
   "source": [
    "# 📌 Catégories sélectionnées\n",
    "SELECTED_CATEGORIES = {\"electronics\", \"computers\", \"sport\", \"kids\"}\n",
    "\n",
    "def load_data_with_duckdb():\n",
    "    \"\"\"Charge et filtre les données en streaming avec DuckDB pour économiser la mémoire.\"\"\"\n",
    "    print(\"📂 Chargement des fichiers Parquet en streaming avec DuckDB...\")\n",
    "\n",
    "    query = f\"\"\"\n",
    "        SELECT \n",
    "            user_id, \n",
    "            event_type, \n",
    "            event_time,\n",
    "            price, \n",
    "            split_part(category_code, '.', 1) AS category_main\n",
    "        FROM read_parquet('{DATA_PATH}*.parquet', hive_partitioning=0)\n",
    "        WHERE event_type IN ('view', 'purchase','cart')\n",
    "        AND split_part(category_code, '.', 1) IN ({', '.join(f\"'{cat}'\" for cat in SELECTED_CATEGORIES)})\n",
    "    \"\"\"\n",
    "\n",
    "    return duckdb.query(query)  # Retourne un objet DuckDB en streaming\n",
    "\n",
    "def generate_user_features():\n",
    "    \"\"\"Transforme les données en jeu de caractéristiques par utilisateur avec DuckDB en optimisant la mémoire.\"\"\"\n",
    "    print(\"🔄 Chargement des données en streaming...\")\n",
    "    df = load_data_with_duckdb()  # Chargement optimisé\n",
    "\n",
    "    print(\"📊 Calcul des métriques par utilisateur...\")\n",
    "\n",
    "    # 📌 Agrégation en plusieurs étapes pour réduire la mémoire utilisée\n",
    "    user_features_query = \"\"\"\n",
    "        SELECT \n",
    "            user_id,\n",
    "            COUNT(*) AS total_events,\n",
    "            COUNT(CASE WHEN event_type = 'view' THEN 1 END) AS total_views,\n",
    "            COUNT(CASE WHEN event_type = 'cart' THEN 1 END) AS total_carts,\n",
    "            COUNT(CASE WHEN event_type = 'purchase' THEN 1 END) AS total_purchases,\n",
    "            SUM(CASE WHEN event_type = 'purchase' THEN price ELSE 0 END) AS total_spent,\n",
    "            COUNT(DISTINCT category_main) AS unique_categories,\n",
    "            MAX(event_time) AS last_event_time\n",
    "        FROM df\n",
    "        GROUP BY user_id\n",
    "    \"\"\"\n",
    "\n",
    "    user_features = duckdb.query(user_features_query).to_df()\n",
    "\n",
    "    print(\"📅 Conversion des dates...\")\n",
    "    # 📌 Conversion des dates (en UTC et naive)\n",
    "    user_features[\"last_event_time\"] = pd.to_datetime(user_features[\"last_event_time\"]).dt.tz_localize(None)\n",
    "\n",
    "    print(\"🔢 Calcul du taux de conversion...\")\n",
    "    # 📌 Calcul du taux de conversion\n",
    "    user_features[\"conversion_rate_view\"] = user_features[\"total_purchases\"] / user_features[\"total_views\"]\n",
    "    user_features[\"conversion_rate_view\"].fillna(0, inplace=True)\n",
    "\n",
    "    print(\"🔢 Calcul du taux de panier transformé...\")\n",
    "    # 📌 Calcul du taux de conversion\n",
    "    user_features[\"conversion_rate_cart\"] = user_features[\"total_purchases\"] / user_features[\"total_carts\"]\n",
    "    user_features[\"conversion_rate_cart\"].fillna(0, inplace=True)\n",
    "\n",
    "    print(\"⏳ Calcul du temps depuis le dernier événement...\")\n",
    "    # 📌 Calcul du temps depuis le dernier événement (en jours)\n",
    "    now_utc = datetime.utcnow()\n",
    "    user_features[\"days_since_last_event\"] = (\n",
    "        now_utc - user_features[\"last_event_time\"]\n",
    "    ).dt.total_seconds() / (3600 * 24)\n",
    "\n",
    "    # 📌 Nettoyage des valeurs infinies et NaN\n",
    "    user_features.replace([float(\"inf\"), float(\"-inf\")], None, inplace=True)\n",
    "    user_features.fillna(0, inplace=True)\n",
    "\n",
    "    # 📌 Sauvegarde avec compression (pour économiser la RAM)\n",
    "    print(f\"💾 Sauvegarde optimisée du fichier {OUTPUT_PATH}...\")\n",
    "    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
    "    user_features.to_parquet(OUTPUT_PATH, engine=\"pyarrow\", compression=\"snappy\")\n",
    "\n",
    "    print(\"✅ Transformation terminée avec succès !\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_user_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Chargement des modèles sauvegardés...\n",
      "✅ Modèles chargés : scaler, PCA, et K-Means.\n",
      "📥 Chargement des nouvelles données...\n",
      "✅ Nouvelles données chargées : 814509 lignes, 11 colonnes.\n",
      "🔄 Normalisation des nouvelles données...\n",
      "✅ Normalisation terminée.\n",
      "🔄 Réduction de dimension avec l'ACP...\n",
      "✅ Réduction de dimension terminée.\n",
      "🎯 Prédiction des clusters...\n",
      "✅ Prédiction des clusters terminée.\n",
      "💾 Sauvegarde des résultats...\n",
      "✅ Résultats sauvegardés dans '../data/output/new_user_clusters-test.parquet'.\n"
     ]
    }
   ],
   "source": [
    "# 🔹 Charger les modèles sauvegardés\n",
    "print(\"📥 Chargement des modèles sauvegardés...\")\n",
    "scaler = joblib.load(\"../data/output/scaler-test.pkl\")\n",
    "pca = joblib.load(\"../data/output/pca_model-test.pkl\")\n",
    "kmeans = joblib.load(\"../data/output/kmeans_model-test.pkl\")\n",
    "print(\"✅ Modèles chargés : scaler, PCA, et K-Means.\")\n",
    "\n",
    "# 🔹 Charger les nouvelles données\n",
    "print(\"📥 Chargement des nouvelles données...\")\n",
    "new_data = pd.read_parquet(\"../data/output/user_features_data-sampled.parquet\")\n",
    "print(f\"✅ Nouvelles données chargées : {new_data.shape[0]} lignes, {new_data.shape[1]} colonnes.\")\n",
    "\n",
    "# 🔹 Normaliser les nouvelles données\n",
    "print(\"🔄 Normalisation des nouvelles données...\")\n",
    "features = [\"total_events\", \"total_views\", \"total_carts\", \"total_purchases\", \n",
    "            \"total_spent\", \"unique_categories\", \"days_since_last_event\"]\n",
    "new_data_scaled = scaler.transform(new_data[features])  # Appliquer le scaler sauvegardé\n",
    "print(\"✅ Normalisation terminée.\")\n",
    "\n",
    "# 🔹 Réduction de dimension avec l'ACP\n",
    "print(\"🔄 Réduction de dimension avec l'ACP...\")\n",
    "new_data_pca = pca.transform(new_data_scaled)  # Appliquer le modèle PCA sauvegardé\n",
    "new_data_pca_df = pd.DataFrame(data=new_data_pca, columns=[\"PC1\", \"PC2\"])\n",
    "new_data_pca_df[\"user_id\"] = new_data[\"user_id\"]\n",
    "print(\"✅ Réduction de dimension terminée.\")\n",
    "\n",
    "# 🔹 Prédire les clusters\n",
    "print(\"🎯 Prédiction des clusters...\")\n",
    "new_data_pca_df[\"Cluster\"] = kmeans.predict(new_data_pca_df[[\"PC1\", \"PC2\"]])  # Appliquer le modèle K-Means\n",
    "print(\"✅ Prédiction des clusters terminée.\")\n",
    "\n",
    "# 🔹 Sauvegarder les résultats\n",
    "print(\"💾 Sauvegarde des résultats...\")\n",
    "new_data_pca_df.to_parquet(\"../data/output/new_user_clusters-test.parquet\", index=False)\n",
    "print(\"✅ Résultats sauvegardés dans '../data/output/new_user_clusters-test.parquet'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Cluster   count\n",
      "0        0   49107\n",
      "1        1     707\n",
      "2        2  764218\n",
      "3        3     477\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "# Chemin vers votre fichier Parquet\n",
    "file_path = \"../data/output/new_user_clusters-test.parquet\"\n",
    "\n",
    "try:\n",
    "    # Établir une connexion à DuckDB\n",
    "    con = duckdb.connect(database=':memory:', read_only=False)\n",
    "\n",
    "    # Exécuter la requête SQL pour faire le group by et compter les lignes\n",
    "    result = con.execute(f\"SELECT Cluster, COUNT(*) AS count FROM '{file_path}' GROUP BY Cluster\").fetchdf()\n",
    "\n",
    "    # Afficher le résultat\n",
    "    print(result)\n",
    "\n",
    "    # Fermer la connexion DuckDB\n",
    "    con.close()\n",
    "\n",
    "except duckdb.Error as e:\n",
    "    print(f\"Une erreur DuckDB s'est produite : {e}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Le fichier '{file_path}' n'a pas été trouvé.\")\n",
    "except Exception as e:\n",
    "    print(f\"Une erreur inattendue s'est produite : {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
