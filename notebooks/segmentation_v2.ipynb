{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pierre\\Desktop\\Evaluation école\\.venv\\Lib\\site-packages\\distributed\\node.py:187: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 61151 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✨ Génération des features utilisateur améliorées...\n",
      " Chargement des fichiers Parquet...\n",
      "✅ Fichiers Parquet chargés et filtrés !\n",
      "⏳ Vérification et conversion du type datetime...\n",
      "✅ Type datetime vérifié et converti !\n",
      " Calcul des métriques de base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 09:13:27,358 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle a11c0cc8d2b4a5df6def6015d45c30b3 initialized by task ('shuffle-transfer-a11c0cc8d2b4a5df6def6015d45c30b3', 12) executed on worker tcp://127.0.0.1:61173\n",
      "2025-03-13 09:13:30,099 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:61182 (pid=7224) exceeded 95% memory budget. Restarting...\n",
      "2025-03-13 09:13:30,305 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle a11c0cc8d2b4a5df6def6015d45c30b3 deactivated due to stimulus 'handle-worker-cleanup-1741853610.291021'\n",
      "2025-03-13 09:13:30,317 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle a11c0cc8d2b4a5df6def6015d45c30b3 restarted due to stimulus 'handle-worker-cleanup-1741853610.291021\n",
      "2025-03-13 09:13:30,672 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-03-13 09:14:18,295 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle a11c0cc8d2b4a5df6def6015d45c30b3 initialized by task ('shuffle-transfer-a11c0cc8d2b4a5df6def6015d45c30b3', 0) executed on worker tcp://127.0.0.1:61173\n",
      "2025-03-13 09:14:35,595 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:61362 (pid=22740) exceeded 95% memory budget. Restarting...\n",
      "2025-03-13 09:14:35,728 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle a11c0cc8d2b4a5df6def6015d45c30b3 deactivated due to stimulus 'handle-worker-cleanup-1741853675.714333'\n",
      "2025-03-13 09:14:35,742 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle a11c0cc8d2b4a5df6def6015d45c30b3 restarted due to stimulus 'handle-worker-cleanup-1741853675.714333\n",
      "2025-03-13 09:14:36,121 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-03-13 09:15:43,060 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:61415 (pid=26048) exceeded 95% memory budget. Restarting...\n",
      "2025-03-13 09:15:44,635 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-03-13 09:15:50,097 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:61179 (pid=28996) exceeded 95% memory budget. Restarting...\n",
      "2025-03-13 09:15:50,115 - distributed.scheduler - WARNING - Removing worker 'tcp://127.0.0.1:61179' caused the cluster to lose already computed task(s), which will be recomputed elsewhere: {('read_parquet-fused-operation-e4aa2ff58ac818a5f27b942dc8f1ff57', 15)} (stimulus_id='handle-worker-cleanup-1741853750.1014516')\n",
      "2025-03-13 09:15:50,652 - distributed.nanny - WARNING - Restarting worker\n",
      "2025-03-13 09:17:21,071 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:61476 (pid=23032) exceeded 95% memory budget. Restarting...\n",
      "2025-03-13 09:17:21,605 - distributed.nanny.memory - WARNING - Worker tcp://127.0.0.1:61476 (pid=23032) is slow to terminate; trying again\n",
      "2025-03-13 09:17:21,894 - distributed.scheduler - ERROR - Task ('read_parquet-fused-operation-e4aa2ff58ac818a5f27b942dc8f1ff57', 24) marked as failed because 4 workers died while trying to run it\n",
      "2025-03-13 09:17:22,338 - distributed.nanny - WARNING - Restarting worker\n"
     ]
    },
    {
     "ename": "KilledWorker",
     "evalue": "Attempted to run task ('read_parquet-fused-operation-e4aa2ff58ac818a5f27b942dc8f1ff57', 24) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:61476. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKilledWorker\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 100\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     99\u001b[39m     client = Client(n_workers=\u001b[32m4\u001b[39m, threads_per_worker=\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     user_features = \u001b[43mgenerate_user_features_enhanced\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m user_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    102\u001b[39m         user_features = cluster_users(user_features)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mgenerate_user_features_enhanced\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Calcul des métriques de base\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m Calcul des métriques de base...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m total_events = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m total_views = df[df[\u001b[33m\"\u001b[39m\u001b[33mevent_type\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mview\u001b[39m\u001b[33m\"\u001b[39m].groupby(\u001b[33m\"\u001b[39m\u001b[33muser_id\u001b[39m\u001b[33m\"\u001b[39m).size().compute()\n\u001b[32m     44\u001b[39m total_purchases = df[df[\u001b[33m\"\u001b[39m\u001b[33mevent_type\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mpurchase\u001b[39m\u001b[33m\"\u001b[39m].groupby(\u001b[33m\"\u001b[39m\u001b[33muser_id\u001b[39m\u001b[33m\"\u001b[39m).size().compute()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Pierre\\Desktop\\Evaluation école\\.venv\\Lib\\site-packages\\dask\\dataframe\\dask_expr\\_collection.py:489\u001b[39m, in \u001b[36mFrameBase.compute\u001b[39m\u001b[34m(self, fuse, concatenate, **kwargs)\u001b[39m\n\u001b[32m    487\u001b[39m     out = out.repartition(npartitions=\u001b[32m1\u001b[39m)\n\u001b[32m    488\u001b[39m out = out.optimize(fuse=fuse)\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDaskMethodsMixin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Pierre\\Desktop\\Evaluation école\\.venv\\Lib\\site-packages\\dask\\base.py:374\u001b[39m, in \u001b[36mDaskMethodsMixin.compute\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    350\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs):\n\u001b[32m    351\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[32m    352\u001b[39m \n\u001b[32m    353\u001b[39m \u001b[33;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    372\u001b[39m \u001b[33;03m    dask.compute\u001b[39;00m\n\u001b[32m    373\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m374\u001b[39m     (result,) = \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    375\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Pierre\\Desktop\\Evaluation école\\.venv\\Lib\\site-packages\\dask\\base.py:662\u001b[39m, in \u001b[36mcompute\u001b[39m\u001b[34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[39m\n\u001b[32m    659\u001b[39m     postcomputes.append(x.__dask_postcompute__())\n\u001b[32m    661\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m     results = \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, *a) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Pierre\\Desktop\\Evaluation école\\.venv\\Lib\\site-packages\\distributed\\client.py:2426\u001b[39m, in \u001b[36mClient._gather\u001b[39m\u001b[34m(self, futures, errors, direct, local_worker)\u001b[39m\n\u001b[32m   2424\u001b[39m     exception = st.exception\n\u001b[32m   2425\u001b[39m     traceback = st.traceback\n\u001b[32m-> \u001b[39m\u001b[32m2426\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception.with_traceback(traceback)\n\u001b[32m   2427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m errors == \u001b[33m\"\u001b[39m\u001b[33mskip\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2428\u001b[39m     bad_keys.add(key)\n",
      "\u001b[31mKilledWorker\u001b[39m: Attempted to run task ('read_parquet-fused-operation-e4aa2ff58ac818a5f27b942dc8f1ff57', 24) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:61476. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from dask.distributed import Client\n",
    "from dask.utils import is_dataframe_like  # Importation corrigée\n",
    "\n",
    "# Configuration\n",
    "DATA_PATH = \"../data/cleaned\"\n",
    "OUTPUT_PATH = \"../output/user_features_enhanced.parquet\"\n",
    "SELECTED_CATEGORIES = {\"electronics\", \"computers\", \"sport\", \"kids\"}\n",
    "CHUNK_SIZE = \"100MB\"  # Ajustez en fonction de votre mémoire\n",
    "\n",
    "def load_parquet_files():\n",
    "    print(\" Chargement des fichiers Parquet...\")\n",
    "    files = glob.glob(os.path.join(DATA_PATH, \"*.parquet\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"Aucun fichier .parquet trouvé dans {DATA_PATH}\")\n",
    "    cols = [\"user_id\", \"event_type\", \"event_time\", \"category_code\"]\n",
    "    df = dd.read_parquet(files, columns=cols, engine=\"pyarrow\", chunksize=CHUNK_SIZE)\n",
    "    df = df[df[\"event_type\"].isin([\"view\", \"purchase\"])]\n",
    "    df[\"category_main\"] = df[\"category_code\"].str.split(\".\").str[0]\n",
    "    df = df[df[\"category_main\"].isin(SELECTED_CATEGORIES)]\n",
    "    print(\"✅ Fichiers Parquet chargés et filtrés !\")\n",
    "    return df\n",
    "\n",
    "def generate_user_features_enhanced():\n",
    "    print(\"✨ Génération des features utilisateur améliorées...\")\n",
    "    df = load_parquet_files()\n",
    "\n",
    "    # Gestion du type datetime\n",
    "    print(\"⏳ Vérification et conversion du type datetime...\")\n",
    "    if not is_dataframe_like(df[\"event_time\"]) or not dd.api.types.is_datetime64_dtype(df[\"event_time\"].dtype):\n",
    "        df[\"event_time\"] = dd.to_datetime(df[\"event_time\"], errors='coerce')\n",
    "    df = df.dropna(subset=[\"event_time\"])\n",
    "    print(\"✅ Type datetime vérifié et converti !\")\n",
    "\n",
    "    # Calcul des métriques de base\n",
    "    print(\" Calcul des métriques de base...\")\n",
    "    total_events = df.groupby(\"user_id\").size().compute()\n",
    "    total_views = df[df[\"event_type\"] == \"view\"].groupby(\"user_id\").size().compute()\n",
    "    total_purchases = df[df[\"event_type\"] == \"purchase\"].groupby(\"user_id\").size().compute()\n",
    "    unique_categories = df.groupby(\"user_id\")[\"category_main\"].nunique().compute()\n",
    "    last_event_time = df.groupby(\"user_id\")[\"event_time\"].max().compute()\n",
    "    print(\"✅ Métriques de base calculées !\")\n",
    "\n",
    "    # Calcul des métriques temporelles\n",
    "    print(\"⏱️ Calcul des métriques temporelles...\")\n",
    "    df[\"month\"] = df[\"event_time\"].dt.month\n",
    "    monthly_purchases = df[df[\"event_type\"] == \"purchase\"].groupby([\"user_id\", \"month\"]).size().unstack(fill_value=0).compute()\n",
    "    time_features = pd.concat([last_event_time, monthly_purchases], axis=1, sort=False).fillna(0)\n",
    "    print(\"✅ Métriques temporelles calculées !\")\n",
    "\n",
    "    # Calcul des préférences par catégorie\n",
    "    print(\" Calcul des préférences par catégorie...\")\n",
    "    category_views = df[df[\"event_type\"] == \"view\"].groupby([\"user_id\", \"category_main\"]).size().unstack(fill_value=0).compute()\n",
    "    category_purchases = df[df[\"event_type\"] == \"purchase\"].groupby([\"user_id\", \"category_main\"]).size().unstack(fill_value=0).compute()\n",
    "    category_features = pd.concat([category_views, category_purchases], axis=1, sort=False).fillna(0)\n",
    "    print(\"✅ Préférences par catégorie calculées !\")\n",
    "\n",
    "    # Fusion de toutes les features\n",
    "    print(\" Fusion de toutes les features...\")\n",
    "    user_features = pd.concat([total_events, total_views, total_purchases, unique_categories, time_features, category_features], axis=1, sort=False).fillna(0)\n",
    "    user_features[\"conversion_rate\"] = user_features[\"total_purchases\"] / user_features[\"total_views\"]\n",
    "    user_features[\"conversion_rate\"] = user_features[\"conversion_rate\"].fillna(0)\n",
    "    print(\"✅ Features fusionnées et taux de conversion calculé !\")\n",
    "\n",
    "    # Sauvegarde en Parquet\n",
    "    print(f\" Sauvegarde des features dans {OUTPUT_PATH}...\")\n",
    "    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
    "    user_features.to_parquet(OUTPUT_PATH, engine=\"pyarrow\")\n",
    "    print(\"✅ Features utilisateur améliorées sauvegardées !\")\n",
    "    return user_features\n",
    "\n",
    "def cluster_users(user_features):\n",
    "    print(\" Clustering des utilisateurs...\")\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(user_features)\n",
    "    kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "    clusters = kmeans.fit_predict(scaled_features)\n",
    "    user_features[\"cluster\"] = clusters\n",
    "    print(\"✅ Clustering terminé !\")\n",
    "    return user_features\n",
    "\n",
    "def analyze_sales_by_cluster(user_features):\n",
    "    print(\" Analyse des ventes par cluster...\")\n",
    "    monthly_sales = user_features.groupby(\"cluster\")[[col for col in user_features.columns if isinstance(col, int)]].sum()\n",
    "    print(\" Ventes mensuelles par cluster :\")\n",
    "    print(monthly_sales)\n",
    "    category_cols = [col for col in user_features.columns if isinstance(col, str) and col in SELECTED_CATEGORIES]\n",
    "    category_sales = user_features.groupby(\"cluster\")[category_cols].sum()\n",
    "    print(\" Ventes par catégorie et par cluster :\")\n",
    "    print(category_sales)\n",
    "    print(\"✅ Analyse des ventes terminée !\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    client = Client(n_workers=4, threads_per_worker=1)\n",
    "    user_features = generate_user_features_enhanced()\n",
    "    if user_features is not None:\n",
    "        user_features = cluster_users(user_features)\n",
    "        analyze_sales_by_cluster(user_features)\n",
    "    client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
